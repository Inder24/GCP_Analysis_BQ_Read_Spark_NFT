{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "Basic Setup - NO need to do on DataProc. But required on fresh notebook\n---------------------------------------------------------------------------------------------------"}, {"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "nd9c1SpM_dbu", "outputId": "3b1143c4-31e1-43fc-b8f5-0ff0926d52e8"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "E: Package 'openjdk-8-jdk-headless' has no installation candidate\r\n"}], "source": "# !apt-get update\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null"}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wjMLXRYWJuHc", "outputId": "39f87841-a43f-4a61-e71c-129607564af4"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "openjdk version \"1.8.0_322\"\r\nOpenJDK Runtime Environment (Temurin)(build 1.8.0_322-b06)\r\nOpenJDK 64-Bit Server VM (Temurin)(build 25.322-b06, mixed mode)\r\n"}], "source": "!java -version"}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8zLrNCZHeqC9", "outputId": "8c3c71a9-f1ee-4619-c036-d36517babcf5"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pyspark in /usr/lib/spark/python (3.1.2)\nRequirement already satisfied: py4j==0.10.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"}], "source": "pip install pyspark"}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "AwV4Gb63_jmu"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n\u001b[0m"}], "source": "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n!pip install -q findspark"}, {"cell_type": "markdown", "metadata": {}, "source": "--------------------------------------------------------------------------------------------------\nBasic PySpark Code below \n------------------------------------------------"}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "Wk8IrzVb_kyU"}, "outputs": [{"ename": "Exception", "evalue": "Unable to find py4j in /content/spark-3.0.0-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n", "\u001b[0;31mIndexError\u001b[0m: list index out of range", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)", "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/spark-3.0.0-bin-hadoop2.7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n", "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.0.0-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"]}], "source": "import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\nimport findspark\nfindspark.init()"}, {"cell_type": "code", "execution_count": 8, "metadata": {"id": "-gqOT05s_vSF"}, "outputs": [{"ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/content/spark-3.0.0-bin-hadoop2.7/./bin/spark-submit'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(spark)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:331\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 331\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/java_gateway.py:98\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, signal\u001b[38;5;241m.\u001b[39mSIG_IGN)\n\u001b[1;32m     97\u001b[0m     popen_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreexec_fn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preexec_func\n\u001b[0;32m---> 98\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-3.0.0-bin-hadoop2.7/./bin/spark-submit'"]}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nsqlContext = SQLContext(spark)"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "y0dmkrlm_5a9", "outputId": "119d1d45-763f-4948-e4b6-04de6aaf96e3"}, "outputs": [], "source": "df = sqlContext.createDataFrame([('2020', \"Jan\", 2000), ('2019', \"Jan\", 3000), ('2018', \"Jan\", 4000)],(\"year\", \"month\", \"sales\"))\ndf.show()"}, {"cell_type": "markdown", "metadata": {"id": "O8yKB5PJ_8h-"}, "source": "------------------------------------------------------------------\nNo need to do on dataproc, but other notebook, yes. \nBasic Setup to connect to gcs (Google Cloud Storage)\n------------------------------------------------------------------------------------"}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "GiaExLQ-AT5d"}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'google.colab'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auth\n\u001b[1;32m      2\u001b[0m auth\u001b[38;5;241m.\u001b[39mauthenticate_user()\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}], "source": "from google.colab import auth\nauth.authenticate_user()"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "mIg42hduAfe0", "outputId": "b528cb05-342e-4a1f-b0e3-feb82fa7bd6c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2537  100  2537    0     0  57873      0 --:--:-- --:--:-- --:--:-- 59000\nOK\n3 packages can be upgraded. Run 'apt list --upgradable' to see them.\nThe following additional packages will be installed:\n  fuse\nThe following NEW packages will be installed:\n  fuse gcsfuse\n0 upgraded, 2 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 12.1 MB of archives.\nAfter this operation, 28.7 MB of additional disk space will be used.\nDo you want to continue? [Y/n] "}], "source": "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n!apt -qq update\n!apt -qq install gcsfuse"}, {"cell_type": "markdown", "metadata": {"id": "3hdiCGs3Ap6k"}, "source": "Installed gcfs to use google cloud storage\n----------------------------------------------------------------------------------\n\nNow We copy file from gcs to either local directory or to hadoop cluster\n--------------------------------------------------------------------------------------------------------"}, {"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "k8J9BSBtCDDc", "outputId": "93bead04-446d-4504-f4aa-59f884e91c9a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "AccessDeniedException: 403 486650843789-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.\r\n"}], "source": "!gsutil cp gs://nft_csv_ingestion/nft_sales.csv /tmp/nft_sales.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!gsutil cp /tmp/nft_sales.csv gs://nft_csv_ingestion/nft_sales.csv "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hadoop dfs -cp  gs://nft_csv_ingestion/nft_sales.csv /tmp/nft_sales.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df = sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").load(\"/tmp/nft_sales.csv\")\ndf.show(5)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wAM-CXNSAw50", "outputId": "0b11af11-55c4-4f48-a217-66c15dbd6e8e"}, "outputs": [], "source": "df = sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\", \",\").load(\"file:///tmp/nft_sales.csv\")\ndf.show(5)\n"}, {"cell_type": "markdown", "metadata": {"id": "UOwE-mYYfyst"}, "source": "\n---------------------------------------------\n\nSql lite db etl analysis\n-------------------------------------\n"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "WARNING: Use of this script to execute dfs is deprecated.\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n\ncp: `/tmp/nfts.sqlite': File exists\n"}], "source": "!hadoop dfs -cp  gs://nft_sql_lite_db/nfts.sqlite /tmp/nfts.sqlite"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying gs://nft_sql_lite_db/nfts.sqlite...\n/ [1 files][  6.9 GiB/  6.9 GiB]   55.0 MiB/s                                   \nOperation completed over 1 objects/6.9 GiB.                                      \n"}], "source": "!gsutil cp gs://nft_sql_lite_db/nfts.sqlite /tmp/nfts.sqlite"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "/tmp/nfts.sqlite\r\n"}], "source": "!ls /tmp/nfts.sqlite"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\r\ndrwxrwxrwt   - hdfs hadoop          0 2022-04-07 06:21 /tmp/hadoop-yarn\r\ndrwx-wx-wx   - hive hadoop          0 2022-04-07 06:21 /tmp/hive\r\n-rw-r--r--   1 root hadoop 7433949184 2022-04-10 16:43 /tmp/nfts.sqlite\r\n"}], "source": "!hadoop fs -ls /tmp"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark import SparkContext\nsc = SparkContext.getOrCreate()\nfrom pyspark.sql import SQLContext\nsqlCtx = SQLContext(sc)\n\nsqlContext.read.format(\"jdbc\").options(url =\"jdbc:sqlite:/tmp/nfts.sqlite\", driver=\"org.sqlite.JDBC\", dbtable=\"current_owners\").load().take(10) "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "myrdd2 = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:sqlite:/tmp/nfts.sqlite\").option(\"dbtable\", \"current_owners\").option(\"driver\", \"org.sqlite.JDBC\").load()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!cd /tmp\n!curl https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///tmp/sqlite-jdbc-3.36.0.3.jar pyspark-shell'"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "driver = \"org.sqlite.JDBC\"\npath = \"/tmp/nfts.sqlite\"\nurl = \"jdbc:sqlite:\" + path\ntablename = \"current_owners\""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_albums = sqlContext.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nspark3 =SparkSession.builder \\\n  .master('local[*]') \\\n  .appName('conversions') \\\n  .config('spark.jars.packages', 'org.xerial:sqlite-jdbc:3.34.0') \\\n  .getOrCreate() \nsqlContext = SQLContext(spark3)"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Python 3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:42:07) \n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\nIvy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\norg.xerial#sqlite-jdbc added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-deda2f06-212d-4c22-9a00-65e42e35fc3b;1.0\n\tconfs: [default]\n\tfound org.xerial#sqlite-jdbc;3.34.0 in central\n:: resolution report :: resolve 161ms :: artifacts dl 3ms\n\t:: modules in use:\n\torg.xerial#sqlite-jdbc;3.34.0 from central in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-deda2f06-212d-4c22-9a00-65e42e35fc3b\n\tconfs: [default]\n\t0 artifacts copied, 1 already retrieved (0kB/5ms)\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/04/10 16:55:58 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/04/10 16:55:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/04/10 16:55:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/04/10 16:55:58 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n22/04/10 16:56:00 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial_sqlite-jdbc-3.34.0.jar added multiple times to distributed cache.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n      /_/\n\nUsing Python version 3.8.12 (default, Jan 30 2022 23:42:07)\nSpark context Web UI available at http://nft-cluster-m.asia-southeast1-a.c.triple-skein-345705.internal:45029\nSpark context available as 'sc' (master = yarn, app id = application_1649608307986_0003).\nSparkSession available as 'spark'.\n>>> \nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 285, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n>>> "}], "source": "!pyspark --packages org.xerial:sqlite-jdbc:3.34.0\n!exit"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/04/10 16:54:36 WARN org.apache.spark.deploy.DependencyUtils: Local jar /content/sqlite-jdbc-3.8.6.jar does not exist, skipping.\n22/04/10 16:54:37 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/04/10 16:54:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/04/10 16:54:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/04/10 16:54:37 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n22/04/10 16:54:37 INFO org.sparkproject.jetty.util.log: Logging initialized @3413ms to org.sparkproject.jetty.util.log.Slf4jLog\n22/04/10 16:54:37 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06\n22/04/10 16:54:37 INFO org.sparkproject.jetty.server.Server: Started @3531ms\n22/04/10 16:54:37 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@4c4776d7{HTTP/1.1, (http/1.1)}{0.0.0.0:33517}\n22/04/10 16:54:37 ERROR org.apache.spark.SparkContext: Failed to add /content/sqlite-jdbc-3.8.6.jar to Spark environment\njava.io.FileNotFoundException: Jar /content/sqlite-jdbc-3.8.6.jar not found\n\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1937)\n\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1991)\n\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n22/04/10 16:54:39 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nspark4 =SparkSession.builder \\\n  .master('local[*]') \\\n  .appName('conversions') \\\n  .config('spark.jars', '/content/sqlite-jdbc-3.8.6.jar') \\\n  .getOrCreate() \nsqlContext = SQLContext(spark4)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": "'/content/sqlite-jdbc-3.8.6.jar'"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark4.conf.get(\"spark.jars\")"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"ename": "Py4JJavaError", "evalue": "An error occurred while calling o81.load.\n: java.lang.ClassNotFoundException: org.sqlite.JDBC\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43morg.sqlite.JDBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcurrent_owners\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjdbc:sqlite:/tmp/nfts.sqlite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:210\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o81.load.\n: java.lang.ClassNotFoundException: org.sqlite.JDBC\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"]}], "source": "df = spark4.read.format('jdbc') \\\n        .options(driver='org.sqlite.JDBC', dbtable='current_owners',\n                 url='jdbc:sqlite:gs:///nfts.sqlite')\\\n        .load()"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "import sqlite3\n\nimport pandas as pd\n\ndb_path = '/tmp/nfts.sqlite'\n\nquery = 'SELECT * from current_owners'\n\nconn = sqlite3.connect(db_path)\n\na_pandas_df = pd.read_sql_query(query, conn)\n\na_spark_df = sqlContext.createDataFrame(data=a_pandas_df)"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/04/11 12:31:47 WARN org.apache.spark.scheduler.TaskSetManager: Stage 0 contains a task of very large size (173673 KiB). The maximum recommended task size is 1000 KiB.\n\r[Stage 0:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------+--------------------+\n|         nft_address|token_id|               owner|\n+--------------------+--------+--------------------+\n|0x00000000000b7F8...|       0|0xb776cAb26B9e6Be...|\n|0x00000000000b7F8...|       1|0x8A73024B39A4477...|\n|0x00000000000b7F8...|      10|0x5e5C817E9264B46...|\n|0x00000000000b7F8...|      11|0x8376f63c13b99D3...|\n|0x00000000000b7F8...|      12|0xb5e34552F32BA92...|\n+--------------------+--------+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "a_spark_df.show(5)"}, {"cell_type": "markdown", "metadata": {"id": "zAxTfgdcCX40"}, "source": "Basic pyspark working in Google Colab"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Collecting nfts\n  Downloading nfts-0.0.2-py3-none-any.whl (16 kB)\nRequirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nfts) (4.63.1)\nCollecting humbug\n  Downloading humbug-0.2.7-py3-none-any.whl (11 kB)\nRequirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from nfts) (1.2.5)\nCollecting web3\n  Downloading web3-5.28.0-py3-none-any.whl (499 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m499.6/499.6 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting moonstreamdb\n  Downloading moonstreamdb-0.2.3-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from nfts) (2.27.1)\nRequirement already satisfied: sqlalchemy in /opt/conda/miniconda3/lib/python3.8/site-packages (from moonstreamdb->nfts) (1.4.32)\nRequirement already satisfied: alembic in /opt/conda/miniconda3/lib/python3.8/site-packages (from moonstreamdb->nfts) (1.7.7)\nCollecting psycopg2-binary\n  Downloading psycopg2_binary-2.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->nfts) (2.8.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->nfts) (2022.1)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->nfts) (1.19.5)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->nfts) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->nfts) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->nfts) (1.25.11)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->nfts) (2021.10.8)\nCollecting jsonschema<4.0.0,>=3.2.0\n  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting hexbytes<1.0.0,>=0.1.0\n  Downloading hexbytes-0.2.2-py3-none-any.whl (6.1 kB)\nCollecting eth-hash[pycryptodome]<1.0.0,>=0.2.0\n  Downloading eth_hash-0.3.2-py3-none-any.whl (8.8 kB)\nCollecting ipfshttpclient==0.8.0a2\n  Downloading ipfshttpclient-0.8.0a2-py3-none-any.whl (82 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m82.6/82.6 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting websockets<10,>=9.1\n  Downloading websockets-9.1-cp38-cp38-manylinux2010_x86_64.whl (102 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m102.3/102.3 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting eth-typing<3.0.0,>=2.0.0\n  Downloading eth_typing-2.3.0-py3-none-any.whl (6.2 kB)\nCollecting lru-dict<2.0.0,>=1.1.6\n  Downloading lru-dict-1.1.7.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting eth-utils<2.0.0,>=1.9.5\n  Downloading eth_utils-1.10.0-py3-none-any.whl (24 kB)\nCollecting eth-account<0.6.0,>=0.5.7\n  Downloading eth_account-0.5.7-py3-none-any.whl (101 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m101.8/101.8 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf<4,>=3.10.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from web3->nfts) (3.19.4)\nCollecting eth-abi<3.0.0,>=2.0.0b6\n  Downloading eth_abi-2.1.1-py3-none-any.whl (27 kB)\nRequirement already satisfied: aiohttp<4,>=3.7.4.post0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from web3->nfts) (3.8.1)\nCollecting multiaddr>=0.0.7\n  Downloading multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (4.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp<4,>=3.7.4.post0->web3->nfts) (1.2.0)\nCollecting parsimonious<0.9.0,>=0.8.0\n  Downloading parsimonious-0.8.1.tar.gz (45 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.1/45.1 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting rlp<3,>=1.0.0\n  Downloading rlp-2.0.1-py2.py3-none-any.whl (20 kB)\nCollecting eth-rlp<2,>=0.1.2\n  Downloading eth_rlp-0.3.0-py3-none-any.whl (5.0 kB)\nCollecting eth-keys<0.4.0,>=0.3.4\n  Downloading eth_keys-0.3.4-py3-none-any.whl (21 kB)\nCollecting bitarray<1.3.0,>=1.2.1\n  Downloading bitarray-1.2.2.tar.gz (48 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m48.2/48.2 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting eth-keyfile<0.6.0,>=0.5.0\n  Downloading eth_keyfile-0.5.1-py3-none-any.whl (8.3 kB)\nCollecting pycryptodome<4,>=3.6.6\n  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cytoolz<1.0.0,>=0.10.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from eth-utils<2.0.0,>=1.9.5->web3->nfts) (0.11.2)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from jsonschema<4.0.0,>=3.2.0->web3->nfts) (0.18.1)\nRequirement already satisfied: setuptools in /opt/conda/miniconda3/lib/python3.8/site-packages (from jsonschema<4.0.0,>=3.2.0->web3->nfts) (59.8.0)\nRequirement already satisfied: six>=1.11.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from jsonschema<4.0.0,>=3.2.0->web3->nfts) (1.16.0)\nRequirement already satisfied: Mako in /opt/conda/miniconda3/lib/python3.8/site-packages (from alembic->moonstreamdb->nfts) (1.2.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/miniconda3/lib/python3.8/site-packages (from alembic->moonstreamdb->nfts) (4.11.3)\nRequirement already satisfied: importlib-resources in /opt/conda/miniconda3/lib/python3.8/site-packages (from alembic->moonstreamdb->nfts) (5.4.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from sqlalchemy->moonstreamdb->nfts) (1.1.2)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from cytoolz<1.0.0,>=0.10.1->eth-utils<2.0.0,>=1.9.5->web3->nfts) (0.11.2)\nCollecting eth-rlp<2,>=0.1.2\n  Downloading eth_rlp-0.2.1-py3-none-any.whl (5.0 kB)\nCollecting base58\n  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\nCollecting netaddr\n  Downloading netaddr-0.8.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting varint\n  Downloading varint-1.0.2.tar.gz (1.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from importlib-metadata->alembic->moonstreamdb->nfts) (3.7.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from Mako->alembic->moonstreamdb->nfts) (2.1.1)\n"}, {"name": "stdout", "output_type": "stream", "text": "Building wheels for collected packages: lru-dict, bitarray, parsimonious, varint\n  Building wheel for lru-dict (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp38-cp38-linux_x86_64.whl size=32617 sha256=f971401cfadac1a0ff8b8ef60c494e7cd2a48d60edded857916cbb61d98b5013\n  Stored in directory: /root/.cache/pip/wheels/ab/ed/82/d47759a6e2416fb78d6ee81c02a970a50d5dc6fc270b5a3242\n  Building wheel for bitarray (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bitarray: filename=bitarray-1.2.2-cp38-cp38-linux_x86_64.whl size=167564 sha256=773f0ff3ef2fed2b5fabbb58f807e60de45847d61228fdaef706484ec062dae2\n  Stored in directory: /root/.cache/pip/wheels/41/36/95/5b4eca059535a8400e8f4ca38f4883ea1801bb221fbd8170df\n  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for parsimonious: filename=parsimonious-0.8.1-py3-none-any.whl size=42723 sha256=09028b2570446eede55379841fa445142b1b612cbbd63d810e654a410021719e\n  Stored in directory: /root/.cache/pip/wheels/d8/af/19/fb896f509a437aca2dcf62583e84d7fb2cd5b628c1564a609c\n  Building wheel for varint (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for varint: filename=varint-1.0.2-py3-none-any.whl size=1980 sha256=d87fcf4a5be0486045be006900720dd7d1fe32bc43c2c1063ecd14a8d4cdeafa\n  Stored in directory: /root/.cache/pip/wheels/cc/a8/a4/4d9e9807c27585dc974fc0e86a3e4345649d71f8a35906d1a8\nSuccessfully built lru-dict bitarray parsimonious varint\nInstalling collected packages: varint, netaddr, lru-dict, bitarray, websockets, pycryptodome, psycopg2-binary, parsimonious, jsonschema, hexbytes, eth-typing, eth-hash, base58, multiaddr, humbug, eth-utils, rlp, moonstreamdb, ipfshttpclient, eth-keys, eth-abi, eth-rlp, eth-keyfile, eth-account, web3, nfts\n  Attempting uninstall: jsonschema\n    Found existing installation: jsonschema 4.4.0\n    Uninstalling jsonschema-4.4.0:\n      Successfully uninstalled jsonschema-4.4.0\nSuccessfully installed base58-2.1.1 bitarray-1.2.2 eth-abi-2.1.1 eth-account-0.5.7 eth-hash-0.3.2 eth-keyfile-0.5.1 eth-keys-0.3.4 eth-rlp-0.2.1 eth-typing-2.3.0 eth-utils-1.10.0 hexbytes-0.2.2 humbug-0.2.7 ipfshttpclient-0.8.0a2 jsonschema-3.2.0 lru-dict-1.1.7 moonstreamdb-0.2.3 multiaddr-0.0.9 netaddr-0.8.0 nfts-0.0.2 parsimonious-0.8.1 psycopg2-binary-2.9.3 pycryptodome-3.14.1 rlp-2.0.1 varint-1.0.2 web3-5.28.0 websockets-9.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}], "source": "!pip install nfts"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "import os\nimport sqlite3\n\nimport matplotlib.pyplot as plt\nimport nfts.dataset\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import curve_fit\nfrom scipy.special import zeta"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "2928c921-5892-4800-9675-c11030c550b1_resources\r\n523ded43-4f7d-4599-b98a-69e58ea31661_resources\r\nSERVERENGINE_SOCKETMANAGER_2022-04-11T12:15:32Z_1900\r\ncluster\r\ndataproc-agent3526609400769203650\r\nfdf96458-ddc5-453b-96e1-5f9ad9292cf0_resources\r\nhadoop-unjar22182705041423930\r\nhadoop-unjar8040116867953293139\r\nhadoop-yarn-yarn\r\nhive\r\nhsperfdata_hdfs\r\nhsperfdata_hive\r\nhsperfdata_knox\r\nhsperfdata_mapred\r\nhsperfdata_root\r\nhsperfdata_spark\r\nhsperfdata_yarn\r\njetty-0_0_0_0-10002-hive-service-3_1_2_jar-_-any-9158813490248169229\r\njetty-0_0_0_0-8042-hadoop-yarn-common-3_2_2_jar-_-any-519076419844954091\r\njetty-0_0_0_0-8088-hadoop-yarn-common-3_2_2_jar-_-any-4291174230845628428\r\njetty-0_0_0_0-8188-hadoop-yarn-common-3_2_2_jar-_-any-5140851581501485200\r\njetty-0_0_0_0-8188-tez-ui-0_9_2_war-_tez-ui-any-7482519492520096315\r\njetty-0_0_0_0-9868-secondary-_-any-4631692437528764687\r\njetty-0_0_0_0-9870-hdfs-_-any-3585418971285134640\r\njetty-localhost-38321-datanode-_-any-3277174947876911983\r\njetty-nft-cluster-m-19888-hadoop-yarn-common-3_2_2_jar-_-any-9196637003286681051\r\nlibleveldbjni-64-1-3941972414900533657.8\r\nlibleveldbjni-64-1-4772829494025924335.8\r\nnfts.sqlite\r\nspark-61cbae97-b164-491f-baf3-7f7f36c5c6e5\r\nsystemd-private-c420da8d43ff4d8e8c97f40f0f2697c4-chrony.service-nLavsc\r\nsystemd-private-c420da8d43ff4d8e8c97f40f0f2697c4-haveged.service-UxQbsj\r\n"}], "source": "!ls /tmp\n"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "DATASET_PATH = \"/tmp/nfts.sqlite\"\n# DATASET_PATH = \"gs://nft_sql_lite_db/nfts.sqlite\""}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "ds = nfts.dataset.FromSQLite(DATASET_PATH)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "current_owners_df = ds.load_dataframe(\"current_owners\")"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'current_owners_df' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcurrent_owners_df\u001b[49m\n", "\u001b[0;31mNameError\u001b[0m: name 'current_owners_df' is not defined"]}], "source": "current_owners_df"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"colab": {"collapsed_sections": [], "name": "Copy of Welcome To Colaboratory", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}